# VideoCaptionerWithVit

A command-line utility to generate timeâ€‘aligned captions for any video by extracting keyâ€‘frames (via Katna or OpenCV) and running them through a pre-trained Vision Transformer (ViT) + GPTâ€‘2 image captioning model.

---

## ğŸš€ Features

- **Keyâ€‘frame extraction**  
  - Primary: [Katna](https://github.com/bhattbhavesh91/Katna) for smart saliency-based frames  
  - Fallback: Uniform sampling via OpenCV  
- **Caption generation**  
  - Uses Hugging Faceâ€™s [`nlpconnect/vit-gpt2-image-captioning`](https://huggingface.co/nlpconnect/vit-gpt2-image-captioning) model under the hood  
- **Multiâ€‘threaded processing** for faster captioning on long videos  
- **Outputs**  
  - `.srt` subtitle file  
  - `.json` with timestamped captions  

---

## ğŸ“¦ Installation

1. Clone this repo:
   ```bash
   git clone https://github.com/<yourâ€‘username>/VideoCaptionerWithVit.git
   cd VideoCaptionerWithVit
   ```
2. Create & activate a virtual environment:
   ```bash
   python3 -m venv venv
   source venv/bin/activate
   ```
3. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

---

## ğŸ› ï¸ Usage

```bash
python vit_captioner_video.py \
  --video_path path/to/your/video.mp4 \
  --num_frames 10
```

- `--video_path` (`-V`): input video file  
- `--num_frames` (`-N`): how many keyâ€‘frames to caption (default: 10)

After running, youâ€™ll get:
- `video_captioning_frames/` (extracted frames)
- `video_caption.srt`
- `video_caption.json`

---

## ğŸ”— Upstream & Inspiration

- **Image captioning model**:  
  [`nlpconnect/vit-gpt2-image-captioning`](https://huggingface.co/nlpconnect/vit-gpt2-image-captioning) by NLP Connect  
- **Keyâ€‘frame extraction**:  
  [Katna](https://github.com/bhattbhavesh91/Katna) for smart saliency-based sampling  
- **Fallback frame sampling**:  
  OpenCVâ€™s `CAP_PROP_POS_FRAMES` uniform slicing  

---

## ğŸ“ Repo Structure

```
.
â”œâ”€â”€ vit_captioner.py           # wraps ViT-GPT2 model for single-image captions
â”œâ”€â”€ vit_captioner_video.py     # video-to-SRT/JSON pipeline
â”œâ”€â”€ keyframes_extractor.py     # Katna-based key-frame extraction
â”œâ”€â”€ data/                      # (optional) place your test videos here
â”œâ”€â”€ frames_output/             # example output folder
â”œâ”€â”€ requirements.txt           # pip dependencies
â””â”€â”€ README.md                  # this file
```

---

## ğŸ¤ Contributing

1. Fork it  
2. Create a feature branch (`git checkout -b feat/YourFeature`)  
3. Commit your changes (`git commit -m 'Add feature'`)  
4. Push to the branch (`git push origin feat/YourFeature`)  
5. Open a Pull Request  

---

## ğŸ“„ License

MIT Â© Lachlan Chen
